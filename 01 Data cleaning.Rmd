---
title: "Airbnb in NYC with SparkR and Sparkling water"
author: "Dominik Klepl"
date: "2/6/2020"
output: html_document
---

## Connect to Spark
Load required libraries
```{r echo=FALSE}
library(sparklyr)
library(rsparkling)
library(dplyr)
library(ggplot2) #for plots
library(ggthemes)
library(magrittr) #pipes support
```

Launch Spark session and create Spark context (sc)
```{r}
sc = spark_connect(master = "local", version = "2.2.1")
```

Create H2O context
```{r}
#hc = H2OContext.getOrCreate(sc)
```

Open H2O (Sparkling water) UI
```{r}
#hc$openFlow()
```

### Load data
Our dataset is a csv file. Now we'll load into spark.
```{r}
#path to dataset
DATA_PATH = "data/AB_NYC_2019.csv"
VISUALIZATIONS = "documentation/figures"
data = spark_read_csv(sc = sc, name = "data", path = DATA_PATH)
class(data)
```

Let's look at the data
```{r}
cat("The dataset has", sdf_nrow(data), "datapoints","\nand", sdf_ncol(data), "features.")
```

What columns do we have?
```{r}
colnames(data)
```

Some of these columns won't be very useful in the further analysis so we exclude them.
```{r}
data = data %>%
  select(-c(id, host_name))
```


## Missing values
First, we'll deal with the missing values and try to avoid dropping them as much as possible.
```{r}
(missing_values = data %>% 
  mutate_all(is.na) %>%
  mutate_all(as.numeric) %>%
  summarise_all(sum) %>% 
  collect() %>%
  t() %>%
  as.data.frame())
  
colnames(missing_values) = "NAs"
missing_values$Feature = rownames(missing_values)
rownames(missing_values) = NULL

missing_values = missing_values %>% filter(NAs !=0)

(NA_plot = ggplot(missing_values, aes(x = Feature, y = NAs, fill=Feature))+
  geom_col()+
  theme_few()+
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())+
  labs(title = "",
       x = "",
       y = "Sum of missing values"))

ggsave(paste0(VISUALIZATIONS,"/missing_values.png"), NA_plot, width = 7, height = 4)
```

Most missing values are in last_review and reviews_per_month. This probably happens when a listing has no reviews yet. Let's test that hypothesis.
```{r}
#how many number_of_reviews==0 have NA in reviews_per_month 
NA_test = data %>% 
  select(number_of_reviews, reviews_per_month, last_review) %>%
  filter(number_of_reviews==0) %>%
  mutate(na_month = as.character(is.na(reviews_per_month))) %>%
  mutate(na_last = as.character(is.na(last_review)))

NA_test %>%
  group_by(na_month) %>%
  summarise(count = n())

NA_test %>%
  group_by(na_last) %>%
  summarise(count = n())
```

Most of NAs in last_review and reviews_per_month were indeed the result of having no reviews yet. We can therefore replace these NAs with 0.
There are also a few missing values in name and host_id, we'll replace these with "unknown" as there might be a use for that column.
Since host_id and host_name contain the same information, we'll exclude the host_name.
```{r}
data = data %>%
  #replace with 0
  mutate(last_review = ifelse(is.na(last_review), 0, last_review)) %>% 
  #replace with 0
  mutate(reviews_per_month = ifelse(is.na(reviews_per_month), 0, reviews_per_month)) %>% 
  #replace with unknown
  mutate(name = ifelse(is.na(name), "Unknown", name)) %>%
  #replace with unknown
  mutate(host_id = ifelse(is.na(host_id), "Unknown", host_id))
```

Now there is only a few missing values left (around 300), we'll simply exclude those datapoints.
```{r}
data = na.omit(data)
```

## Categorical features
Make sure that the categories are normalized (e.g. all lowercase)

#### Neighbourhood group
```{r}
data %>%
  distinct(neighbourhood_group) %>%
  collect()

#there are some strange categories - some number and "D"
#inspect how many listings there are per neighbourhood group
neighbourhoods_groups = data %>%
  group_by(neighbourhood_group) %>%
  summarise(count = n()) %>%
  filter(count > 1) %>%
  collect()

#there are several neighbourhoods with single listing, let's remove those - create list of neighbourhoods to keep, we'll filter later, all at once
neighbourhood_gr_keep = neighbourhoods_groups$neighbourhood_group
```

#### Neighbourhood
```{r}
data %>%
  distinct(neighbourhood) %>%
  arrange(neighbourhood) %>% #order alphabetically
  mutate(neighbourhood = tolower(neighbourhood)) %>%
  collect()

#there's many neighbourhoods but again we can see some strange names and it's also possible there will be neighbourhoods with very low number of listings - let's exclude all that have less than 50
(neighbourhoods = data %>%
  group_by(neighbourhood) %>%
  summarise(count = n()) %>%
  filter(count >= 50) %>%
  arrange(desc(count)) %>%
  collect())

#that leaves us with 98 neighbourhoods - let's save those, filter later
neighbourhood_keep = neighbourhoods$neighbourhood
```

#### Room type
```{r}
data %>%
  distinct(room_type)

#again, we see strange values, let's get only those that have more than 1 listing
(room_types = data %>% 
  group_by(room_type) %>%
  summarise(count = n()) %>%
  filter(count > 1) %>%
  collect())

#let's get a vector of room_types to keep
room_keep = room_types$room_type
```

#### Filter categories
```{r}
data_clean = data %>%
  filter(neighbourhood_group %in% neighbourhood_gr_keep) %>%
  filter(neighbourhood %in% neighbourhood_keep) %>%
  filter(room_type %in% room_keep)

sdf_nrow(data_clean)
```

## Save to csv
Normally, we'd save the data back to HDFS (or any other distributed storage), however we know that this dataset can fit in the memory, so we'll simply collect it from the workers and write it to csv the "classic" way. Sorry.
```{r}
write = collect(data_clean)
write.csv(write, "data/cleaned_data.csv")
```



